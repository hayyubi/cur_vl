MODEL:
  # This is what indicates we want image-caption training not object detection
  META_ARCHITECTURE: "MMSS-RCNN"
  # URL to the initial weights, trained for imagenet classification
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-50"
  BACKBONE:
    # don't freeze any layer, train everything
    FREEZE_CONV_BODY_AT: 0
  LANGUAGE_BACKBONE:
    # make a BERT model to process captions
    TYPE: "BERT-Base"
    # If not using pretrained, then unfreeze  
    USE_PRETRAINED: True
    # and freeze it (loaded from original pretrained bert of huggingface)
    FREEZE: True
  BBOX:
    # Whether to use ground truth (GT) or automatic region proposal (RP).
    TYPE: "GT"
  MMSS_HEAD:
    # We want both a grounding head and a transformer head on top of image and caption,
    # each of which defines its own objective functions.
    TYPES: ("GroundingHead", "TransformerHead")
    DEFAULT_HEAD: "GroundingHead"
    # Share the weights of the vision to language projection between the two heads. 
    # Use the one on the grounding head because that is the default (see above)
    TIE_VL_PROJECTION_WEIGHTS: True
    # Randomly keep up to 100 visual regions from each image. This is to save memory.
    SPATIAL_DROPOUT: 100
   # Scale Transformer Head loss as it's auxilary
    SCALE_TH: -1.0
    GROUNDING:
      # Use dot product for grounding. This could be cosine or euclidean too.
      LOCAL_METRIC: "dot"
      # After aligning words to regions, sum the local distances to compute global distance.
      GLOBAL_METRIC: "aligned_local"
      # Use softmax to softly align each word to regions, and vice versa. 
      # This could be for instance hardmax, which aligns to the most similar
      ALIGNMENT: "softmax"
      # If we want to align keeping curriculum in mind. Can be either current_model, previous_model
      # or "".
      ALIGNMENT_CURRICULUM: "current_model"
      # Typical good values are 100.0 for euclidean, 10.0 for dot, 0.01 for cosine
      ALIGNMENT_TEMPERATURE: 10.0
      # This loss is to choose the right caption out of all captions in the batch, 
      # And similarly choose the right image. 
      # Could be: cross_entropy, triplet, curriculum_aware_triplet. 
      # If curriculum_aware_triplet, then global_metric and alginment are ignored. 
      LOSS: "cross_entropy"
      # Whether to find a region for each word
      ALIGN_WORDS_TO_REGIONS: True
      # Whether to find a word for a region
      # At least one of these two should be True
      ALIGN_REGIONS_TO_WORDS: True
    TRANSFORMER:
      # Whether to perform masked language modeling (randomly mask words from captions
      # and have the model reconstruct them)
      MASKED_LANGUAGE_MODELING: True
      # Whether to do that during validation as well. That is not good if you want to
      # measure image-caption matching scores.
      MASKED_LANGUAGE_MODELING_VALIDATION: False
      # For now this is not implemented, so keep it False and ''
      MASKED_VISUAL_MODELING: False
      MVM_LOSS: ''
      # For Multimedia Matching loss, cross-entropy works just like in the grounding head
      MMM_LOSS: 'cross_entropy'
      # Typical BERT configs as in Huggingface
      BERT_CONFIG:
        num_hidden_layers: 6
        num_attention_heads: 8
        intermediate_size: 768
DATASETS:
  TRAIN: ("coco_cap_det_noun_phrase_train",)
  TEST: ("coco_cap_det_noun_phrase_val",) # "visual_genome_filtered",)
  DATASET_ARGS: 
    # load embeddings from the annotation json. Note it doesn't work with original COCO json.
    # First run ipynb/003.ipynb or ipynb/004.ipynb to add embeddings to annotations
    # During testing, we need to turn it on
    LOAD_EMBEDDINGS: True
    # The key for embedding to load. We have BertEmb and GloVE for now.
    EMB_KEY: "BertEmb"
    # Dimension of embeddings (300 for Glove, 768 for Bert)
    EMB_DIM: 768
    # Reduce dataset size while training.
    REDUCE_TO_PERCENT: -1.0
    # Whether to use noun and noun phrase tokens
    WORD_N_NOUN_PHRASE: True
SOLVER:
  BASE_LR: 0.00250 # (0.01 / 8 ) * 2
  WEIGHT_DECAY: 0.0001
  STEPS: (80000, 140000) # (20000 * 8/2, 35000 * 8/2)
  MAX_ITER: 160000 # 40000 * ( 8 / 2)
  IMS_PER_BATCH: 16 
  TEST_PERIOD: 1000
  CHECKPOINT_PERIOD: 1000
  LOG_PERIOD: 100
  CLIP_GRAD_NORM_AT: 5.0
  # A value of more than one means accumulate gradients for several batches before updating
  GRADIENT_ACCUMULATION_STEPS: 1
  # If true, it calls model.train() before computing validation loss. Needed for some models.
  USE_TRAIN_MODE_FOR_VALIDATION_LOSS: False
TEST:
  DO_EVAL: False
  IMS_PER_BATCH: 16
CURRICULUM: 
 DO: False
 NUM_PHASES: 4
 ITERS: (4000, 18000, 56000, 82000)
 # Hard coded file path containing division of phases
 PHASES_FILE: "/home/hammad/gaila/data/coco/coco_stats/spacy/3_noun/img_caps_per_noun_list.pkl" 