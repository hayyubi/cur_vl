{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latin-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optional-strengthening",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-formation",
   "metadata": {},
   "source": [
    "## Convert captions to nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "executive-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = '../datasets/coco/annotations/captions_train2017.json'\n",
    "noun_only_annotations_file = '../datasets/coco/annotations/captions_noun_only_train2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "scheduled-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(annotations_file, 'r'))\n",
    "\n",
    "for ann in dataset['annotations']:\n",
    "    caption = ann['caption']\n",
    "    noun_set = set()\n",
    "    for tok in nlp(caption):\n",
    "        if 'NN' in tok.tag_:\n",
    "            noun_set.add(str(tok))\n",
    "                \n",
    "    noun_str = ' '.join(list(noun_set))\n",
    "    ann['caption'] = noun_str\n",
    "    \n",
    "with open(noun_only_annotations_file, 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-canadian",
   "metadata": {},
   "source": [
    "## Investigate Bert tokenizer tokenization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from maskrcnn_benchmark.modeling.language_backbone.transformers import BERT\n",
    "from maskrcnn_benchmark.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "industrial-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BERT(cfg)\n",
    "_ = bert.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "clear-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Humility is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "metric-planner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hum', '##ility', 'is']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fancy-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14910, 15148, 2003]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "greatest-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = bert([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "thousand-expense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 14910, 15148,  2003,   102]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "increasing-glenn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hum', '##ility', 'is', '[SEP]']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(enc['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "detailed-celtic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-journal",
   "metadata": {},
   "source": [
    "### How to club words in a phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "northern-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['honda', '#', 'motorcycle']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('honda#motorcycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-airfare",
   "metadata": {},
   "source": [
    "## Club Noun phrase testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "pressing-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a brown stuffed teddy bear wearing a red knitted hat\"\n",
    "tokens = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "single-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a brown stuffed teddy bear\n",
      "a red knitted hat\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    print(np.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "clean-owner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : DT\n",
      "brown : JJ\n",
      "stuffed : VBN\n",
      "teddy : NN\n",
      "bear : NN\n",
      "--------------\n",
      "a : DT\n",
      "red : JJ\n",
      "knitted : VBN\n",
      "hat : NN\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    for t in nlp(np.text):\n",
    "        print(t, ':', t.tag_)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-residence",
   "metadata": {},
   "source": [
    "## Club Noun phrase in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "presidential-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = '../datasets/coco/annotations/captions_val2017.json'\n",
    "noun_phrase_annotations_file = '../datasets/coco/annotations/captions_noun_phrase_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "charged-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(annotations_file, 'r'))\n",
    "\n",
    "for ann in dataset['annotations']:\n",
    "    caption = ann['caption']\n",
    "    noun_set = set()\n",
    "    for np in nlp(caption).noun_chunks:\n",
    "        noun_words = list()\n",
    "        for tok in nlp(np.text):\n",
    "            if 'NN' in tok.tag_:\n",
    "                noun_words.append(str(tok))\n",
    "                \n",
    "        noun_words_str = '#'.join(noun_words)\n",
    "        noun_set.add(str(noun_words_str))\n",
    "                \n",
    "    noun_str = ' '.join(list(noun_set))\n",
    "    ann['caption'] = noun_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "metropolitan-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(noun_phrase_annotations_file, 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-kingdom",
   "metadata": {},
   "source": [
    "## Club Noun phrases while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "mighty-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "turkish-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = '../datasets/coco/annotations/captions_val2017.json'\n",
    "noun_only_annotations_file = '../datasets/coco/annotations/captions_noun_phrase_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "portuguese-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black Honda motorcycle parked in front of a garage.\n",
      "Honda#motorcycle front garage\n",
      "A Honda motorcycle parked in a grass driveway\n",
      "Honda#motorcycle grass#driveway\n",
      "An office cubicle with four different types of computers.\n",
      "types computers office#cubicle\n",
      "A small closed toilet in a cramped space.\n",
      "toilet space\n",
      "Two women waiting at a bench next to a street.\n",
      "street women bench\n",
      "A black Honda motorcycle with a dark burgundy seat.\n",
      "Honda#motorcycle burgundy#seat\n",
      "A tan toilet and sink combination in a small room.\n",
      "sink#combination room toilet\n",
      "The home office space seems to be very cluttered.\n",
      "home#office#space\n",
      "A beautiful dessert waiting to be shared by two people\n",
      "people dessert\n",
      "A woman sitting on a bench and a woman standing waiting for the bus.\n",
      "bus woman bench\n"
     ]
    }
   ],
   "source": [
    "dataset = json.load(open(annotations_file, 'r'))\n",
    "\n",
    "for ann in dataset['annotations'][:10]:\n",
    "    caption = ann['caption']\n",
    "    noun_set = set()\n",
    "    for np in nlp(caption).noun_chunks:\n",
    "        noun_words = list()\n",
    "        for tok in nlp(np.text):\n",
    "            if 'NN' in tok.tag_:\n",
    "                noun_words.append(str(tok))\n",
    "                \n",
    "        noun_words_str = '#'.join(noun_words)\n",
    "        noun_set.add(str(noun_words_str))\n",
    "                \n",
    "    noun_str = ' '.join(list(noun_set))\n",
    "    ann['caption'] = noun_str\n",
    "    \n",
    "    print(caption)\n",
    "    print(noun_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "norwegian-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Honda#motorcycle humility wedding#cake burgundy#seat#humility#tok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "smart-lithuania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['honda',\n",
       " '#',\n",
       " 'motorcycle',\n",
       " 'hum',\n",
       " '##ility',\n",
       " 'wedding',\n",
       " '#',\n",
       " 'cake',\n",
       " 'burgundy',\n",
       " '#',\n",
       " 'seat',\n",
       " '#',\n",
       " 'hum',\n",
       " '##ility',\n",
       " '#',\n",
       " 'to',\n",
       " '##k']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "strong-oxide",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['honda-motorcycle',\n",
       " '#',\n",
       " 'motorcycle',\n",
       " 'hum##ility-cake',\n",
       " '##ility',\n",
       " 'wedding',\n",
       " '#',\n",
       " 'cake',\n",
       " 'burgundy',\n",
       " '#',\n",
       " 'seat',\n",
       " '#',\n",
       " 'hum##ility',\n",
       " '##ility',\n",
       " '#',\n",
       " 'to##k',\n",
       " '##k']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([1 for _ in range(len(tokens))])\n",
    "phrase_stack = []\n",
    "word_stack = []\n",
    "word_start_idx = 0\n",
    "phrase_start_idx = 0\n",
    "\n",
    "for idx, t in enumerate(tokens):\n",
    "    # Processing words\n",
    "    ## If intermediate sub-word\n",
    "    if t[:2] == '##':\n",
    "        word_stack.append(t)\n",
    "        continue\n",
    "    else:\n",
    "        ## stack is empty; add the current token to stack\n",
    "        if len(word_stack) == 0:\n",
    "            word_stack.append(t)\n",
    "            start_word_idx = idx\n",
    "            if len(tokens) > 1 and tokens[idx+1][:2] == '##':\n",
    "                continue\n",
    "        ## stack has a full word previously; remove it and add the current word in expectation of it being\n",
    "        ## the first part of sub-word\n",
    "        elif len(word_stack) == 1:\n",
    "            word_stack.pop()\n",
    "            word_stack.append(t)\n",
    "            start_word_idx = idx\n",
    "        ## Else we need to club the subwords\n",
    "        else:\n",
    "            sub_words = word_stack\n",
    "            tokens[start_word_idx] = ''.join(sub_words)\n",
    "            mask[start_word_idx:idx] = 0\n",
    "            word_stack = [t]\n",
    "            \n",
    "    current_word_idx = start_word_idx\n",
    "    start_word_idx = idx\n",
    "            \n",
    "    # Processing phrase\n",
    "    if current_word_idx == 0:\n",
    "        phrase_stack.append(current_word_idx)\n",
    "        continue\n",
    "        \n",
    "    if t == '#':\n",
    "        continue\n",
    "    \n",
    "    if tokens[current_word_idx-1] == '#':\n",
    "        phrase_stack.append(current_word_idx)\n",
    "    else:\n",
    "        if len(phrase_stack) == 1:\n",
    "            phrase_stack.pop()\n",
    "            phrase_stack.append(current_word_idx)\n",
    "        else:\n",
    "            words_to_combine = phrase_stack\n",
    "            w = []\n",
    "            for i in words_to_combine:\n",
    "                mask[i] = 0\n",
    "                w.append(tokens[i])\n",
    "            tokens[words_to_combine[0]] = '-'.join(w)\n",
    "            phrase_stack = [current_word_idx]\n",
    "\n",
    "if len(word_stack) > 1:\n",
    "    sub_words = word_stack\n",
    "    tokens[start_word_idx] = ''.join(sub_words)\n",
    "    mask[start_word_idx:idx] = 0\n",
    "    word_stack = [t]\n",
    "        \n",
    "current_word_idx = start_word_idx\n",
    "start_word_idx = idx\n",
    "        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "valid-mitchell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [] \n",
    "len(a) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "flush-nancy",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only assign an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-9483ce194000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only assign an iterable"
     ]
    }
   ],
   "source": [
    "a = [0 for _ in range(5)]\n",
    "a[:2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "afraid-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['giraffes', 'person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "assisted-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_batch = tokenizer.batch_encode_plus(text_list, \n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bound-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 21025, 27528, 7959, 2015, 102], [101, 2711, 102, 0, 0, 0]]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "alpha-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "running-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "structured-tribe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'gi', '##raf', '##fe', '##s', '[SEP]']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 21025, 27528, 7959, 2015, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "regulated-collapse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21025, 27528, 7959, 2015]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('giraffes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-preservation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
