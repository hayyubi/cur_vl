{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-082041cee3fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_bert import BasicTokenizer\n",
    "import nltk\n",
    "import spacy \n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = SequenceTagger.load('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Noun Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A metallic oven stove top under a microwave oven.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('metallic', 'JJ'),\n",
       " ('oven', 'RB'),\n",
       " ('stove', 'VBP'),\n",
       " ('top', 'JJ'),\n",
       " ('under', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('microwave', 'NN'),\n",
       " ('oven', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "nltk.pos_tag(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : DT\n",
      "metallic : JJ\n",
      "oven : JJ\n",
      "stove : VBP\n",
      "top : NN\n",
      "under : IN\n",
      "a : DT\n",
      "microwave : NN\n",
      "oven : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(text)\n",
    "for t in tokens:\n",
    "    print(t, ':', t.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top\n",
      "a microwave oven\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "metallic\n",
      "oven\n",
      "stove\n",
      "top\n",
      "under\n",
      "a\n",
      "microwave\n",
      "oven\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    print(str(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with English and German sentences\n",
    "sentence = Sentence(tokenizer.tokenize(text))\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A <DT> metallic <JJ> oven <JJ> stove <NN> top <NN> under <IN> a <DT> microwave <NN> oven <NN> . <.>\n"
     ]
    }
   ],
   "source": [
    "print(\"A <DT> metallic <JJ> oven <JJ> stove <NN> top <NN> under <IN> a <DT> microwave <NN> oven <NN> . <.>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Young married couple preparing to cut white wedding cake.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('young', 'JJ'),\n",
       " ('married', 'VBD'),\n",
       " ('couple', 'JJ'),\n",
       " ('preparing', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('cut', 'VB'),\n",
       " ('white', 'JJ'),\n",
       " ('wedding', 'VBG'),\n",
       " ('cake', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "nltk.pos_tag(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young : NNP\n",
      "married : VBD\n",
      "couple : NN\n",
      "preparing : VBG\n",
      "to : TO\n",
      "cut : VB\n",
      "white : JJ\n",
      "wedding : NN\n",
      "cake : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(text)\n",
    "for t in tokens:\n",
    "    print(t, ':', t.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young\n",
      "couple\n",
      "white wedding cake\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with English and German sentences\n",
    "sentence = Sentence(tokenizer.tokenize(text))\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young <JJ> married <JJ> couple <NN> preparing <VBG> to <TO> cut <VB> white <JJ> wedding <NN> cake <NN> . <.>\n"
     ]
    }
   ],
   "source": [
    "print(\"Young <JJ> married <JJ> couple <NN> preparing <VBG> to <TO> cut <VB> white <JJ> wedding <NN> cake <NN> . <.>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a brown stuffed teddy bear wearing a red knitted hat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('stuffed', 'NN'),\n",
       " ('teddy', 'JJ'),\n",
       " ('bear', 'IN'),\n",
       " ('wearing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('red', 'JJ'),\n",
       " ('knitted', 'VBD'),\n",
       " ('hat', 'WP')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "nltk.pos_tag(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : DT\n",
      "brown : JJ\n",
      "stuffed : VBN\n",
      "teddy : NN\n",
      "bear : NN\n",
      "wearing : VBG\n",
      "a : DT\n",
      "red : JJ\n",
      "knitted : VBN\n",
      "hat : NN\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(text)\n",
    "for t in tokens:\n",
    "    print(t, ':', t.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a brown stuffed teddy bear\n",
      "a red knitted hat\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with English and German sentences\n",
    "sentence = Sentence(tokenizer.tokenize(text))\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a <DT> brown <JJ> stuffed <VBN> teddy <JJ> bear <NN> wearing <VBG> a <DT> red <JJ> knitted <JJ> hat <NN>\n"
     ]
    }
   ],
   "source": [
    "print(\"a <DT> brown <JJ> stuffed <VBN> teddy <JJ> bear <NN> wearing <VBG> a <DT> red <JJ> knitted <JJ> hat <NN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Noun Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A surfer with a paddle catches a wave.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('surfer', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('paddle', 'JJ'),\n",
       " ('catches', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('wave', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "nltk.pos_tag(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : DT\n",
      "surfer : NN\n",
      "with : IN\n",
      "a : DT\n",
      "paddle : NN\n",
      "catches : VBZ\n",
      "a : DT\n",
      "wave : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(text)\n",
    "for t in tokens:\n",
    "    print(t, ':', t.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with English and German sentences\n",
    "sentence = Sentence(tokenizer.tokenize(text))\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A <DT> surfer <NN> with <IN> a <DT> paddle <NN> catches <VBZ> a <DT> wave <NN> . <.>\n"
     ]
    }
   ],
   "source": [
    "print(\"A <DT> surfer <NN> with <IN> a <DT> paddle <NN> catches <VBZ> a <DT> wave <NN> . <.>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a bird that is sitting on a tub full of water.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('bird', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('sitting', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('tub', 'JJ'),\n",
       " ('full', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('water', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "nltk.pos_tag(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : DT\n",
      "bird : NN\n",
      "that : WDT\n",
      "is : VBZ\n",
      "sitting : VBG\n",
      "on : IN\n",
      "a : DT\n",
      "tub : NN\n",
      "full : JJ\n",
      "of : IN\n",
      "water : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(text)\n",
    "for t in tokens:\n",
    "    print(t, ':', t.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bird\n",
      "a tub\n",
      "water\n"
     ]
    }
   ],
   "source": [
    "for np in tokens.noun_chunks:\n",
    "    print(np.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with English and German sentences\n",
    "sentence = Sentence(tokenizer.tokenize(text))\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a <DT> bird <NN> that <WDT> is <VBZ> sitting <VBG> on <IN> a <DT> tub <NN> full <JJ> of <IN> water <NN> . <.>\n"
     ]
    }
   ],
   "source": [
    "print(\"a <DT> bird <NN> that <WDT> is <VBZ> sitting <VBG> on <IN> a <DT> tub <NN> full <JJ> of <IN> water <NN> . <.>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
